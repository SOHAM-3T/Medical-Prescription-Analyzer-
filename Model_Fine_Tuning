{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12916977,"sourceType":"datasetVersion","datasetId":8173329},{"sourceId":12916983,"sourceType":"datasetVersion","datasetId":8173332}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# Step 1: Install a Known Stable Environment (Definitive Method)\n# ==============================================================================\nprint(\"Installing a specific, known-stable set of libraries for a clean environment...\")\n\n# --- FIX: A single, unified install command with pinned versions ---\n# This forces the installer to find a compatible solution for all libraries at once,\n# which is the most robust way to prevent the dependency conflicts we saw earlier.\n!pip install \\\n    torch==2.1.0 \\\n    torchvision==0.16.0 \\\n    torchaudio==2.1.0 \\\n    transformers==4.35.2 \\\n    datasets==2.15.0 \\\n    accelerate==0.25.0 \\\n    bitsandbytes==0.41.2 \\\n    peft==0.7.1 \\\n    evaluate \\\n    jiwer \\\n    sentencepiece \\\n    pillow\n\nimport torch\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom PIL import Image\nimport os\nimport json\nimport numpy as np \nimport re\nimport evaluate \nimport gc \n\nprint(\"Installation and imports complete. Environment is now stable.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:42:45.245933Z","iopub.execute_input":"2025-09-02T10:42:45.246474Z","iopub.status.idle":"2025-09-02T10:45:42.147656Z","shell.execute_reply.started":"2025-09-02T10:42:45.246447Z","shell.execute_reply":"2025-09-02T10:45:42.147012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 2: Configuration and Paths\n# ==============================================================================\n\n# --- IMPORTANT: UPDATE THESE PATHS ---\n# This should point to the folder containing your uploaded images (p1.jpg, etc.)\nIMAGE_DIR = \"/kaggle/input/prescription-data-set\" \n# This should point to the ground_truth_seed.txt file we created\nGROUND_TRUTH_FILE = \"/kaggle/input/ground-truth-seed/ground_truth.txt\"\n\n# This is where your new, custom model will be saved\nOUTPUT_MODEL_DIR = \"/kaggle/working/donut-finetuned-prescription-ocr\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:18.719559Z","iopub.execute_input":"2025-09-02T10:47:18.720820Z","iopub.status.idle":"2025-09-02T10:47:18.724497Z","shell.execute_reply.started":"2025-09-02T10:47:18.720786Z","shell.execute_reply":"2025-09-02T10:47:18.723803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 3: Load and Prepare the Dataset\n# ==============================================================================\n\ndef load_dataset_from_file(image_dir, gt_file_path):\n    print(f\"Loading dataset from {gt_file_path}...\")\n    dataset_dict = {\"image_path\": [], \"ground_truth\": []}\n    \n    with open(gt_file_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split(',', 1)\n            if len(parts) == 2:\n                filename, text = parts\n                image_path = os.path.join(image_dir, filename)\n                if os.path.exists(image_path):\n                    dataset_dict[\"image_path\"].append(image_path)\n                    dataset_dict[\"ground_truth\"].append(text)\n                else:\n                    print(f\"Warning: Image file not found and will be skipped: {image_path}\")\n\n    print(f\"Loaded {len(dataset_dict['image_path'])} images.\")\n    return Dataset.from_dict(dataset_dict)\n\nfull_dataset = load_dataset_from_file(IMAGE_DIR, GROUND_TRUTH_FILE)\n\nif len(full_dataset) > 1:\n    dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = dataset_split[\"train\"]\n    eval_dataset = dataset_split[\"test\"]\nelse:\n    train_dataset = full_dataset\n    eval_dataset = full_dataset\n\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Evaluation set size: {len(eval_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:23.334214Z","iopub.execute_input":"2025-09-02T10:47:23.334789Z","iopub.status.idle":"2025-09-02T10:47:23.404774Z","shell.execute_reply.started":"2025-09-02T10:47:23.334766Z","shell.execute_reply":"2025-09-02T10:47:23.404057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 4: Load Model and Processor\n# ==============================================================================\n\nprint(\"Loading pre-trained Donut model and processor...\")\nprocessor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n\ntask_start_token = \"<s_transcription>\"\ntask_end_token = \"</s_transcription>\"\nprocessor.tokenizer.add_special_tokens({\"additional_special_tokens\": [task_start_token, task_end_token]})\nmodel.decoder.resize_token_embeddings(len(processor.tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:27.267572Z","iopub.execute_input":"2025-09-02T10:47:27.267891Z","iopub.status.idle":"2025-09-02T10:47:40.717920Z","shell.execute_reply.started":"2025-09-02T10:47:27.267868Z","shell.execute_reply":"2025-09-02T10:47:40.717302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 5: Preprocess Data for the Model\n# ==============================================================================\n\nclass DonutDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, processor, max_length=384, split=\"train\"): \n        self.dataset = dataset\n        self.processor = processor\n        self.max_length = max_length\n        self.split = split\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image = Image.open(item['image_path']).convert(\"RGB\")\n        \n        # Aggressive resize to avoid OOM\n        image = image.resize((800, 600))\n        \n        # ðŸ‘‡ FIX: make sure pixel_values is a torch.Tensor, not numpy\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n\n        target_sequence = f\"{task_start_token}{item['ground_truth']}{task_end_token}\"\n        \n        tokenized_output = self.processor.tokenizer(\n            target_sequence,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        \n        labels = tokenized_output.input_ids.squeeze()\n        labels[labels == processor.tokenizer.pad_token_id] = -100\n        \n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\nprocessed_train_dataset = DonutDataset(train_dataset, processor)\nprocessed_eval_dataset = DonutDataset(eval_dataset, processor, split=\"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:44.741514Z","iopub.execute_input":"2025-09-02T10:47:44.742171Z","iopub.status.idle":"2025-09-02T10:47:44.748746Z","shell.execute_reply.started":"2025-09-02T10:47:44.742146Z","shell.execute_reply":"2025-09-02T10:47:44.747867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 6: Define Evaluation Metrics + Custom Collator\n# ==============================================================================\n\nprint(\"Setting up evaluation metrics (Word Error Rate)...\")\nwer = evaluate.load(\"wer\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    if isinstance(logits, tuple):\n        logits = logits[0]\n    pred_ids = np.argmax(logits, axis=-1)\n    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}\n\n# ðŸ‘‡ Custom collator to handle torch tensors safely\nclass DonutCollator:\n    def __init__(self, processor):\n        self.processor = processor\n\n    def __call__(self, features):\n        pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n        labels = torch.stack([f[\"labels\"] for f in features])\n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:49.936024Z","iopub.execute_input":"2025-09-02T10:47:49.936343Z","iopub.status.idle":"2025-09-02T10:47:50.566810Z","shell.execute_reply.started":"2025-09-02T10:47:49.936321Z","shell.execute_reply":"2025-09-02T10:47:50.566025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fix decoder_start_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(task_start_token)\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_MODEL_DIR,\n    num_train_epochs=10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",          # <-- do not save checkpoints at all\n    logging_steps=50,\n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),\n    remove_unused_columns=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_train_dataset,\n    eval_dataset=processed_eval_dataset,\n    tokenizer=processor.tokenizer,\n    data_collator=DonutCollator(processor),   # ðŸ‘ˆ fix here\n    compute_metrics=compute_metrics,\n)\n\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"\\nStarting model fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning complete!\")\n\nprint(f\"Saving fine-tuned model to {OUTPUT_MODEL_DIR}\")\nmodel.save_pretrained(OUTPUT_MODEL_DIR)\nprocessor.save_pretrained(OUTPUT_MODEL_DIR)\nprint(\"Process complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:56.340689Z","iopub.execute_input":"2025-09-02T10:47:56.341006Z","iopub.status.idle":"2025-09-02T10:52:24.316719Z","shell.execute_reply.started":"2025-09-02T10:47:56.340987Z","shell.execute_reply":"2025-09-02T10:52:24.316093Z"}},"outputs":[],"execution_count":null}]}