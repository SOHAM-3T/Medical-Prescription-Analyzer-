{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12916977,
          "sourceType": "datasetVersion",
          "datasetId": 8173329
        },
        {
          "sourceId": 12916983,
          "sourceType": "datasetVersion",
          "datasetId": 8173332
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Fine Tuning Model - Gamma",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SOHAM-3T/Medical-Prescription-Analyzer-/blob/main/Fine_Tuning_Model_Gamma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "0j7DrTFd0LMN"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "soham3ripathy_prescription_data_set_path = kagglehub.dataset_download('soham3ripathy/prescription-data-set')\n",
        "soham3ripathy_ground_truth_seed_path = kagglehub.dataset_download('soham3ripathy/ground-truth-seed')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "FglCgAPt0LMV"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 1: Install a Known Stable Environment (Definitive Method)\n",
        "# ==============================================================================\n",
        "print(\"Installing a specific, known-stable set of libraries for a clean environment...\")\n",
        "\n",
        "# --- FIX: A single, unified install command with pinned versions ---\n",
        "# This forces the installer to find a compatible solution for all libraries at once,\n",
        "# which is the most robust way to prevent the dependency conflicts we saw earlier.\n",
        "!pip install \\\n",
        "    torch==2.1.0 \\\n",
        "    torchvision==0.16.0 \\\n",
        "    torchaudio==2.1.0 \\\n",
        "    transformers==4.35.2 \\\n",
        "    datasets==2.15.0 \\\n",
        "    accelerate==0.25.0 \\\n",
        "    bitsandbytes==0.41.2 \\\n",
        "    peft==0.7.1 \\\n",
        "    evaluate \\\n",
        "    jiwer \\\n",
        "    sentencepiece \\\n",
        "    pillow\n",
        "\n",
        "import torch\n",
        "from transformers import DonutProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import evaluate\n",
        "import gc\n",
        "\n",
        "print(\"Installation and imports complete. Environment is now stable.\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:42:45.245933Z",
          "iopub.execute_input": "2025-09-02T10:42:45.246474Z",
          "iopub.status.idle": "2025-09-02T10:45:42.147656Z",
          "shell.execute_reply.started": "2025-09-02T10:42:45.246447Z",
          "shell.execute_reply": "2025-09-02T10:45:42.147012Z"
        },
        "id": "uxhiM3pW0LMY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 2: Configuration and Paths\n",
        "# ==============================================================================\n",
        "\n",
        "# --- IMPORTANT: UPDATE THESE PATHS ---\n",
        "# This should point to the folder containing your uploaded images (p1.jpg, etc.)\n",
        "IMAGE_DIR = \"/kaggle/input/prescription-data-set\"\n",
        "# This should point to the ground_truth_seed.txt file we created\n",
        "GROUND_TRUTH_FILE = \"/kaggle/input/ground-truth-seed/ground_truth.txt\"\n",
        "\n",
        "# This is where your new, custom model will be saved\n",
        "OUTPUT_MODEL_DIR = \"/kaggle/working/donut-finetuned-prescription-ocr\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:47:18.719559Z",
          "iopub.execute_input": "2025-09-02T10:47:18.72082Z",
          "iopub.status.idle": "2025-09-02T10:47:18.724497Z",
          "shell.execute_reply.started": "2025-09-02T10:47:18.720786Z",
          "shell.execute_reply": "2025-09-02T10:47:18.723803Z"
        },
        "id": "JIegFUg00LMa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3: Load and Prepare the Dataset\n",
        "# ==============================================================================\n",
        "\n",
        "def load_dataset_from_file(image_dir, gt_file_path):\n",
        "    print(f\"Loading dataset from {gt_file_path}...\")\n",
        "    dataset_dict = {\"image_path\": [], \"ground_truth\": []}\n",
        "\n",
        "    with open(gt_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',', 1)\n",
        "            if len(parts) == 2:\n",
        "                filename, text = parts\n",
        "                image_path = os.path.join(image_dir, filename)\n",
        "                if os.path.exists(image_path):\n",
        "                    dataset_dict[\"image_path\"].append(image_path)\n",
        "                    dataset_dict[\"ground_truth\"].append(text)\n",
        "                else:\n",
        "                    print(f\"Warning: Image file not found and will be skipped: {image_path}\")\n",
        "\n",
        "    print(f\"Loaded {len(dataset_dict['image_path'])} images.\")\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "full_dataset = load_dataset_from_file(IMAGE_DIR, GROUND_TRUTH_FILE)\n",
        "\n",
        "if len(full_dataset) > 1:\n",
        "    dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = dataset_split[\"train\"]\n",
        "    eval_dataset = dataset_split[\"test\"]\n",
        "else:\n",
        "    train_dataset = full_dataset\n",
        "    eval_dataset = full_dataset\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation set size: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:47:23.334214Z",
          "iopub.execute_input": "2025-09-02T10:47:23.334789Z",
          "iopub.status.idle": "2025-09-02T10:47:23.404774Z",
          "shell.execute_reply.started": "2025-09-02T10:47:23.334766Z",
          "shell.execute_reply": "2025-09-02T10:47:23.404057Z"
        },
        "id": "H5oliN260LMc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 4: Load Model and Processor\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Loading pre-trained Donut model and processor...\")\n",
        "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "\n",
        "task_start_token = \"<s_transcription>\"\n",
        "task_end_token = \"</s_transcription>\"\n",
        "processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [task_start_token, task_end_token]})\n",
        "model.decoder.resize_token_embeddings(len(processor.tokenizer))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:47:27.267572Z",
          "iopub.execute_input": "2025-09-02T10:47:27.267891Z",
          "iopub.status.idle": "2025-09-02T10:47:40.71792Z",
          "shell.execute_reply.started": "2025-09-02T10:47:27.267868Z",
          "shell.execute_reply": "2025-09-02T10:47:40.717302Z"
        },
        "id": "GC3kxVot0LMd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 5: Preprocess Data for the Model\n",
        "# ==============================================================================\n",
        "\n",
        "class DonutDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, processor, max_length=384, split=\"train\"):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = Image.open(item['image_path']).convert(\"RGB\")\n",
        "\n",
        "        # Aggressive resize to avoid OOM\n",
        "        image = image.resize((800, 600))\n",
        "\n",
        "        # ðŸ‘‡ FIX: make sure pixel_values is a torch.Tensor, not numpy\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
        "\n",
        "        target_sequence = f\"{task_start_token}{item['ground_truth']}{task_end_token}\"\n",
        "\n",
        "        tokenized_output = self.processor.tokenizer(\n",
        "            target_sequence,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels = tokenized_output.input_ids.squeeze()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "processed_train_dataset = DonutDataset(train_dataset, processor)\n",
        "processed_eval_dataset = DonutDataset(eval_dataset, processor, split=\"validation\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:47:44.741514Z",
          "iopub.execute_input": "2025-09-02T10:47:44.742171Z",
          "iopub.status.idle": "2025-09-02T10:47:44.748746Z",
          "shell.execute_reply.started": "2025-09-02T10:47:44.742146Z",
          "shell.execute_reply": "2025-09-02T10:47:44.747867Z"
        },
        "id": "tp-4aUAp0LMe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 6: Define Evaluation Metrics + Custom Collator\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Setting up evaluation metrics (Word Error Rate)...\")\n",
        "wer = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    pred_ids = np.argmax(logits, axis=-1)\n",
        "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}\n",
        "\n",
        "# ðŸ‘‡ Custom collator to handle torch tensors safely\n",
        "class DonutCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "\n",
        "    def __call__(self, features):\n",
        "        pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
        "        labels = torch.stack([f[\"labels\"] for f in features])\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:47:49.936024Z",
          "iopub.execute_input": "2025-09-02T10:47:49.936343Z",
          "iopub.status.idle": "2025-09-02T10:47:50.56681Z",
          "shell.execute_reply.started": "2025-09-02T10:47:49.936321Z",
          "shell.execute_reply": "2025-09-02T10:47:50.566025Z"
        },
        "id": "-Q9jwtic0LMh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix decoder_start_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(task_start_token)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_MODEL_DIR,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",          # <-- do not save checkpoints at all\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train_dataset,\n",
        "    eval_dataset=processed_eval_dataset,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=DonutCollator(processor),   # ðŸ‘ˆ fix here\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nStarting model fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "print(f\"Saving fine-tuned model to {OUTPUT_MODEL_DIR}\")\n",
        "model.save_pretrained(OUTPUT_MODEL_DIR)\n",
        "processor.save_pretrained(OUTPUT_MODEL_DIR)\n",
        "print(\"Process complete.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-02T10:47:56.340689Z",
          "iopub.execute_input": "2025-09-02T10:47:56.341006Z",
          "iopub.status.idle": "2025-09-02T10:52:24.316719Z",
          "shell.execute_reply.started": "2025-09-02T10:47:56.340987Z",
          "shell.execute_reply": "2025-09-02T10:52:24.316093Z"
        },
        "id": "1MXNxIqJ0LMj"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}